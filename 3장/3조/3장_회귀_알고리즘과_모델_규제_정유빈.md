### 용어
- 다중공정성 : 변수간의 상관관계
- 회귀 : 두 변수 사이의 상관관계를 분석하는 방법(독립변수를 가지고 종속변수 찾기)
- 모델 파라미터 : 선형 회귀가 찾은 가중치처럼 머신러닝 모델이 특성에서 학습한 파라미터
- 하이퍼 파라미터 : 사용자가 지정한 값

# 3-1. k-최근접 이웃 회귀  

- 사이킷런에 사용할 훈련 세트는 2차원 배열이어야 한다.  

- 넘파이는 배열의 크기를 자동으로 지정하는 기능 제공(크기에 -1 지정시, 나머지 원소 개수로 모두 채우라는 의미)  
  ex) 첫 번째 크기를 나머지 원소 개수로 채우고, 두 번째 크기를 1로 하려면 train_input.reshape(-1, 1) 사용

## 결정계수(R^2)
- R^2 = 1 - (타깃 - 예측)^2 의 합 / (타깃 - 평균)^2 의 합
- 1 - SSE/SST = SSR
- SST = SSR(설명이 가능) + SSE(설명X)
- 즉, SSR/SSR = R^2 의 값이 1에 가까울 수록 이 모델이 모두 설명할 수 있다고 해석된다.
- 평균 정도를 예측하는 수준이라면, R^2은 0에 가까워지고, 예측이 타깃에 가까워지면 1에 가까워진다.
  
#### score() 메서드의 출력값의 의미  
: score() 메서드가 에러율을 반환한다면 이를 음수를 만들어  
실제로는 낮은 에러가 score()메서드로 반환될 때는 높은 값이 되도록 바꿉니다.

#### bias(편향) 와 variance(분산)중 어떤것이 높을수록 안 좋을까?
: variance가 높을수록 더 안 좋다  
모델의 복잡성 때문에 -> 해결법: 가벼운 모델을 사용한다. 

#### 과소적합이 일어나는 이유
훈련 세트와 테스트 세트의 크기가 매우 작기 때문에


# 3-2. 선형 회귀
- 시간과 환경이 변화하면서 데이터도 바뀌기 때문에 새로운 데이터를 사용해 반복적으로 훈련해야 한다.

## 선형 회귀  
: 직선의 위치가 훈련 세트의 평균에 가깝다면 R^2는 0에 가까운 값이 된다.  
but, 예측을 반대로 하면 R^2은 음수가 될 수 있다. 

<img width="80%" src="https://github.com/kw-chi-community/CHIC_24_machine-learning-study/assets/73346564/0e444b61-72f8-4784-a987-70a7de402400"/>

- LinearRegression 클래스가 찾은 기울기 a 와 y절편 b는 lr 객체의 coef_ 와 intercept_ 속성에 저장되어 있다.
- coef_ 속성 이름에서 알 수 있듯이 머신러닝에서 기울기를 종종 계수(coefficient) 또는 가중치(weigt)라고 부른다.

## 다항 회귀
: 다항식을 사용하여 특성과 타깃 사이의 관계를 나타낸다. 이 함수는 비선형일 수 있지만 선형 회귀로 표현할 수 있다. 
- 2차 방정식의 그래프를 그리려면 제곱한 항이 훈련 세트에 추가 되어야 한다.  
ex. 원래 특성인 길이를 제곱하여 추가했기 때문에 훈련 세트와 테스트 세트 모두 열이 2개로 늘어났다.  
주목할 점은, 2차 방정식 그래프를 찾기 위해 훈련 세트에 제곱 항을 추가했지만, 타깃값은 그대로 사용한다는 것이다. 목표하는 값은 어떤 그래프를 훈련하든지 바꿀 필요가 없다. 단, 제곱한 값과 원래 길이를 함께 넣어주어야 한다. 

> #### 2차 방정식도 선형 회귀라고 하나요?
> 제곱한 길이를 간단히 다른 변수로 치환하여서 진행하면 선형 관계로 표현 할 수 있다.

# 3-3 특성 공학과 규제  
- 특성이 많은 고차원에서는 선형 회귀가 매우 복잡한 모델이 될 수 있다. 


