# 3장 회귀 알고리즘과 모델 규제
## K-최근접 이웃 회귀
K-최근접 이웃 알고리즘이 회귀에도 작동함.

분류와 똑같이 예측하려는 샘플에 가장 가까운 샘플 K개를 선택함. 다른 점은 이웃한 **샘플의 타깃**은 어떤 클래스가 아니라 **임의의 수치**임.

## reshape()
파이썬의 numpy를 이용 시, **배열 차원(Dimension)을 재구조화 및 변경**하고자 할 때 사용. 매개변수로 변경하고자 하는 배열의 행과 열의 차원을 정수로 입력해주면 됨.

- **매개변수 '-1'** 이 의미하는 바는 변경된 배열의 '-1' 위치의 차원은 **"원래 배열의 길이와 남은 차원으로부터 추정"** 이 된다는 뜻
- reshape(-1, 정수) : 정수에 따라서 원소가 해당 열 개수만큼 자동으로 구조화
- reshape(정수, -1) : 행의 정수만큼 행이 생성되어 자동으로 구조화
- reshape(-1) : 1차원 배열 반환

**다만 조건으로는 원소의 개수와 reshape 안의 행과 열의 조합을 통한 행렬의 원소 개수가 동일해야 함.**

예를 들어 np.arrange(7).reshape(2,4)라고 한다면 원소는 7개인데 재구조화하려는 행렬은 2x4 행렬. 즉, 8개의 원소를 필요로 하기 때문에 생성이 안되고 오류가 발생.

+) 차원, 형상이라고 볼 수 있음. 머신러닝보다는 딥러닝에서 자주 쓰이며, 그 중에서도 컴퓨터비전 분야에서 많이 쓰임. resize랑 비슷한 역할을 하며, pytorch의 view()가 있음.

### 딥러닝에서의 reshape()
pytorch의 reshape()과 view() 둘 다 Tensor의 모양을 변경하는 데에 사용될 수 있음. 그러나 둘 사이엔 약간의 차이가 존재함.

- **reshape()** : 가능하면 input의 view를 반환하고, 안되면 contiguous한 tensor로 copy하고 view를 반환한다.
- **view()** : view는 기존의 데이터와 같은 메모리 공간을 공유하며 stride 크기만 변경하여 보여주기만 다르게 한다. 그래서 contigious해야만 동작하며, 아닌 경우 에러를 발생함.

view는 메모리가 기존 Tensor와 동일한 메모리를 공유하는게 보장되지만 reshape은 그렇지 않음.
- 안전하게 형태만 바꾸고 싶다면 **reshape()**
- 메모리가 공유되어 업데이트에 대한 보장이 이루어지고 싶다면 **view**

[+) 참고할만한 사이트](https://subinium.github.io/pytorch-Tensor-Variable/)

## 결정계수(R²)
**R² = 1 - {(타깃 - 예측)²의 합 / (타깃 - 평균)²의 합}** = SSE/SST = 1 - (SSR/SST)

변수간 영향을 주는 정도 또는 인과관계의 정도를 정량화해서 나타낸 수치. 따라서, **결정계수**는 상관분석이 아닌 **회귀분석에서 사용하는 수치**라고 할 수 있음.

각 타깃의 평균 정도를 예측하는 수준이라면 (즉 분자와 분모가 비슷해져) R²은 0에 가까워지고, 예측이 타깃에 아주 가까워지면 (분자가 0에 가까워지기 때문에) 1에 가까운 값이 됨.

- SST = SSR(설명가능) + SSE(모델로서는 설명 불가능, 직선으로 설명X)
회귀의 결정계수

    -> SSR/SST(고정), 우리 모델로 에러 설명 가능.
- 모델로서 설명할 수 없는 게 낮아야 R² 좋음.
- 왜 음수가 나올까? 모델이 평균적으로 맞춘 것에 비해 모델 성능이 낮을 때 음수가 나옴. ȳ(y의 평균값)보다 우리 모델이 못 맞출 때 음수가 나옴.

- 회귀모델에서 독립변수가 종속변수를 얼마나 잘 설명해주는지 보여주는 지표
- 결정계수가 높을수록 독립변수가 종속변수를 잘 설명한다는 뜻이지만, **이때 독립변수의 개수가 증가하면 함께 증가함.**
- 결정계수에만 의존하여 회귀모델을 평가하기에는 무리가 있음. 따라서 **조정된 결정계수**(Adjusted R-squared)가 제시됨.
- 0~1 사이의 값을 나타내지만, 음수값이 나오기도 함.(식을 생각해보면 이해 가능)
- 1에 가깝다고 무조건 좋은 건 아님. 상황마다 다름.

### 조정된 결정계수(Adjusted R-squared)
결정계수는 독립변수 개수가 증가하면 함께 증가하므로 **독립변수 개수가 2개 이상일 경우 Adjusted R-squared를 사용해야함.**

## bias와 variance
편향과 분산은 항상 trade-off 관계. 하나가 커지면 하나는 작아지기 때문에 최적의 값을 찾아줘야함.

bias : 모델 예측 오차

variance : 모델 예측의 편차

bias가 낮고 variance도 낮으면 좋음. **but, 둘 다 큰 경우가 생겼을 경우 뭐가 더 최악일까?(worst case)** 생각해보자.

variance가 낮으면 더 안좋음. bias가 높으면 영점 조절만 하면 됨. but, variance가 높으면 사람이 문제임. variance는 모델의 복잡성이라고도 부름. 일반적으로 bias는 높더라도 variance를 낮추는 방향으로 가는 것이 좋음.