240201 스터디 5장 중 요약

 Decision Tree Algorithm
- 분류와 회귀 모두 적용 가능한 지도 학습 알고리즘
- 많은 머신러닝 알고리즘들이 최적의 가중치를 찾는 과정이었지만,\
  의사결정트리는 간단히 스무고개 놀이를 떠올리면 좋다.

Q. 그렇다면 사용자가 준비한 데이터 피처를 어떤 방식으로 분류하는가? 


CART Algorithm
- Classification and Regression Tree의 약자로, 분류/회귀 모두에 적용 가능함.
- 분류는 불순도(gini/entrophy), 회귀는 MSE(평균제곱오차)가 낮아지는 방향으로 분기 지속

+) 참고
- 불순도(Impurity)는 해당 범주 내에 상이한 데이터가 섞여있는 정도로,
  다양한 개체가 섞여 있을수록 불순도가 높으며 종류로는 Gini, Entrophy가 있다.
- 정보 이득(Information Gain)은 분기 전후의 불순도 차이를 의미한다.
- 각 분할에서 정보 이득을 최대화 수 있도록
- 최종 결과는 동일하더라도 질문(분할)을 어떻게 하느냐에 따라 정보 이득이 다른데,
  이해하기 쉬운 예시로, 1~100 up&down을 할 때 답이 98인데 90을 부르면 \
  첫 질문에 90개를 거르는 상당한 효과를 보게 된다.(수식적 계산보다는 맥락 상의 예시랄까?)

추가로, 장단점을 알아보자.
- 직관적인 해석이 쉽다(정말로다가).
- 피처의 스케일링이 불필요하다.
- 하지만 과적합의 위험이 매우 높다. 이는 분류 기준(gini 또는 entrophy)이나 max_depth, min_samples_split 등의 하이퍼파라미터를
  어떻게 가져가느냐에 따라 완화할 수 있는데, 이것이 '가지치기'이다.  
  
 +) 지니 불순도와 지니 인덱스로 두 개의 식이 있는 개념은 후에 정리해서 올리겠다.
